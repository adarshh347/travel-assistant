import json
from typing import TypedDict, Literal, Optional, List, Any

from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END, START
# non-deployable
# from backend.app.core.config import settings
# from backend.app.services.weather_service import get_weather_multi
# from backend.app.models.dashboard import DashboardResponse

# deployable
from app.core.config import settings
from app.services.weather_service import get_weather_multi
from app.models.dashboard import DashboardResponse


# -------------------------
# 1. Define State
# -------------------------
class AgentState(TypedDict):
    messages: List[Any]  # Chat History
    dashboard_plan: Optional[dict]  # The structure generated by LLM
    weather_report: Optional[List]  # The real data fetched by API
    response_type: Literal["text", "dashboard"]  # Flag for Frontend


# -------------------------
# 2. Setup LLM
# -------------------------
llm = ChatGroq(
    temperature=0,
    model_name="llama-3.3-70b-versatile",
    groq_api_key=settings.GROQ_API_KEY,
)

# We bind the model structure. This allows the LLM to choose:
# 1. Output plain text (for chat)
# 2. Output JSON (for planning)
llm_with_structure = llm.bind_tools([DashboardResponse])

SYSTEM_PROMPT = """
You are a sophisticated Japanese Travel Assistant (AI Travel Agent).

DECISION RULES:
1. If the user asks for a travel plan, suggests a location, or asks "Where should I go?", you MUST call the 'DashboardResponse' tool.
2. If the user asks a follow-up question, reply with standard text in polite Japanese.

DASHBOARD GENERATION RULES:
- **Language**: ALL output must be in natural, polite Japanese (Keigo).
- **Fashion Advice**: Write a DETAILED, helpful paragraph (approx 80-100 words) explaining specifically why certain clothes are needed based on temperature and rain chance.
- **Fashion Styles**: Suggest exactly 3 distinct styles (e.g., Casual, Smart-Casual, Active).
- **Places**: Suggest 3 diverse spots.
- **Nearby Cities**: List 3 major nearby cities.
"""

# -------------------------
# 3. Node: The Planner (LLM)
# -------------------------
async def planner_node(state: AgentState):
    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state["messages"]

    # Invoke LLM with the bound tool
    response = await llm_with_structure.ainvoke(messages)

    return {"messages": [response]}


# -------------------------
# 4. Node: The Fetcher (API)
# -------------------------
async def api_fetcher_node(state: AgentState):
    """
    This node only runs if the LLM generated a Dashboard plan.
    It takes the city names from the plan and fetches real weather.
    """
    last_message = state["messages"][-1]

    # Extract the arguments passed to the tool call
    tool_call = last_message.tool_calls[0]
    dashboard_data = tool_call["args"]  # This is the Dictionary of the plan

    # 1. Extract cities (Main location + Nearby)
    # Ensure inputs are clean strings
    main_city = dashboard_data.get("location")
    nearby = dashboard_data.get("nearby_cities", [])

    all_cities = [main_city] + nearby

    # 2. Call your existing Weather Service
    # (This runs efficiently in parallel)
    real_weather_data = await get_weather_multi(all_cities)

    # 3. Update State
    return {
        "dashboard_plan": dashboard_data,
        "weather_report": real_weather_data,
        "response_type": "dashboard"
    }


# -------------------------
# 5. Conditional Logic (Router)
# -------------------------
def router(state: AgentState):
    """
    Check the last message.
    - If it has tool_calls matching 'DashboardResponse', go to API fetcher.
    - Otherwise, it's just text, so END.
    """
    last_message = state["messages"][-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        # Check if the tool called is our Dashboard schema
        if last_message.tool_calls[0]["name"] == "DashboardResponse":
            return "fetch_api"

    return "end_chat"


# -------------------------
# 6. Build the Graph
# -------------------------
workflow = StateGraph(AgentState)

workflow.add_node("planner", planner_node)
workflow.add_node("fetcher", api_fetcher_node)

# Entry Point
workflow.add_edge(START, "planner")

# Conditional Edge
workflow.add_conditional_edges(
    "planner",
    router,
    {
        "fetch_api": "fetcher",
        "end_chat": END
    }
)

# Fetcher always ends the flow
workflow.add_edge("fetcher", END)

app = workflow.compile()


# -------------------------
# 7. Main Callable
# -------------------------
async def get_chat_response(user_input: str):
    inputs = {
        "messages": [HumanMessage(content=user_input)],
        "response_type": "text",  # Default to text
        "dashboard_plan": None,
        "weather_report": None
    }

    # Run the graph
    final_state = await app.ainvoke(inputs)

    # Format output for Frontend
    if final_state["response_type"] == "dashboard":
        return {
            "type": "dashboard",
            "data": final_state["dashboard_plan"],
            "weather_report": final_state["weather_report"]
        }
    else:
        # It was a normal text chat
        return {
            "type": "text",
            "response": final_state["messages"][-1].content
        }